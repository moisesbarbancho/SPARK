Tarea spark (11 de septiembre)
Como tarea final del curso, el alumno implementará un pequeño motor de ingesta de datos de vuelos en formato JSON. Se proporciona un esqueleto donde la mayor parte del motor está ya implementada, y el alumno solo necesita completar puntos concretos, siguiendo las pistas que se indican en los comentarios. Debéis clonar el repositorio siguiente, y seguir las instrucciones indicadas en el README del repositorio:

https://github.com/dataengineeringucm/spark-tarea-final/

La cantidad de código que falta por completar es pequeña, y las pistas indican cada paso, pero la idea es entender el armazón y probar el funcionamiento de la propuesta completa. Puede resolverse completamente en el portátil del alumno porque los datos utilizados son pequeños, o utilizando databricks-connect con algún IDE (os recomiendo esta última opción porque además, os hace experimentar con dicha herramienta). Yo particularmente utilizo pycharm pero podéis utilizar VS Code o cualquier otro IDE. El paquete databricks-connect no puede ser sustituido por ningún plugin, ya que no ofrecen la misma funcionalidad. La idea es que el wheel generado en vuestro ordenador, lo instaléis en vuestro cluster de Databricks, para probarlo junto al notebook, que también debéis subirlo a vuestro Workspace.

IMPORTANTE: si utilizáis databricks-connect durante el desarrollo, aseguraos de crear un entorno virtual y ELIMINAR la dependencia pyspark del fichero requirements.txt. En su lugar, deben descomentarse las 3 dependencias que vienen comentadas. El motivo es que databricks-connect ya incluye una versión particular modificada de pyspark, que es la única que funciona para conectarse a un cluster remoto de Databricks. Encontraréis los detalles para configurar databricks-connect en la sección de Documentación del curso. En cualquier caso, incluso si utilizáis databricks-connect, no puede ser obligatorio tenerlo instalado para poder ejecutar el motor de ingesta. Por ese motivo, no debe haber ningún import del paquete databrick-connect de manera incondicional: si se utilizase para crear la DatabricksSession, se debe hacer el import dentro de un "if" que compruebe el valor de la propiedad "EXECUTION_ENVIRONMENT" del JSON de configuración. Puede parecer raro meter un import dentro del código, pero de esta manera no dará error nunca si un usuario que no tiene databricks-connect instalado desea ejecutar el motor en modo "producción", puesto que databricks-connect es una herramienta puramente para el desarrollo, que no tiene por qué estar instalada en un entorno productivo (ni debemos obligar a ello).

Como resultado, el alumno debe entregar **dos** ficheros diferentes, separados, **sin indicar el nombre del alumno** en el nombre del fichero: 

- Fichero wheel con el paquete de Python generado, respetando exactamente el nombre que ya se indica en el setup.py. El alumno debe revisar dicho fichero para completar el resto de campos (sin tocar el nombre del paquete). Por tanto el fichero debe llamarse  obligatoriamente motor_ingesta-0.1.0-py3-none-any.whl
- Fichero del notebook resuelto, que debe llamarse obligatoriamente actividad_spark.ipynb. No está permitido resolver en el notebook ninguna operación con los datos. El motor de ingesta como tal se resuelve en el paquete de Python que estáis creando. El notebook es solamente una excusa para probar que el paquete funciona, pero en ningún caso debe implementarse funcionalidad en el notebook.

Yo instalaré vuestro paquete y ejecutaré vuestro notebook tal como habéis hecho vosotros. Podéis utilizar para la gestión del proyecto / creación del fichero wheel cualquier herramienta de Python, como poetry, setuptools o la que queráis experimentar, o no utilizar ninguna en particular y crear el wheel con "python setup.py" (obsoleta) o con "python -m build" tras haber instalado el paquete build en vuestro entorno virtual (pip install build).

No os preocupéis por el nombre del alumno, ya que al descargar todas las entregas, la plataforma del campus automáticamente genera un fichero ZIP dividido en carpetas llamadas cada una con el nombre del alumno en la plataforma, y en dicha carpeta se encuentran los ficheros que hayáis subido, con lo que no hay confusión posible.


Un saludo
