{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f54335-6014-461b-a1fb-546e8851574f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Un motor de ingesta sencillo para el dataset de vuelos\n",
    "\n",
    "### Recuerda borrar todas las líneas que dicen `raise NotImplementedError`\n",
    "\n",
    "El trabajo final consiste en la implementación guiada de un esqueleto de motor de ingesta para ficheros JSON recibidos diariamente con información de los vuelos que han tenido lugar en el día indicado en el nombre del fichero entre dos aeropuertos de los Estados Unidos.\n",
    "* La estructura de cada JSON puede consultarse abriendo con un editor de texto cualquiera de los ficheros.\n",
    "* El significado de cada una de las columnas puede consultarse en el fichero config.json que se encuentra en la carpeta config del repositorio.\n",
    "\n",
    "Vamos a probar nuestro paquete del motor de ingesta. Antes de ejecutar este notebook, es imprescindible haber completado todo el código del motor de ingesta. \n",
    "El notebook solamente valida que el código del motor de ingesta con el cual se ha generado el paquete sea correcto. Para ello, el orden en el que debemos leer, entender y completar los ficheros del repositorio es:\n",
    "\n",
    "1. Clase `MotorIngesta` en el fichero `motor_ingesta.py`. Sólo hay que completar la función `ingesta_fichero`\n",
    "2. (Opcional) Completar los tests `test_aplana` y `test_ingesta_fichero` en el fichero `test_ingesta.py`\n",
    "3. Clase `FlujoDiario` en el fichero `flujo_diario.py`. Completar primero el inicializador y luego el método `procesa_diario`. Al ir completando el código de este método, veremos que a su vez necesitamos completar las dos funciones siguientes.\n",
    "4. Función `aniade_hora_utc` en el fichero `agregaciones.py`\n",
    "5. (Opcional) El test de dicha función se llama  `test_aniade_hora_utc` en el fichero `test_ingesta.py`\n",
    "6. Función `aniade_intervalos_por_aeropuerto` en el fichero `agregaciones.py`\n",
    "7. (Opcional) El test de dicha función se llama `test_aniade_intervalos_por_aeropuerto` en el fichero `test_ingesta.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d54713de-6613-41d7-8d3c-551b9ef39ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install loguru==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165573f0-25b3-462f-a40f-503bf8cf3b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instalamos el wheel en el cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da8b683-0b39-4c65-98cb-cc19e497a487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall ../dist/motor_ingesta-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f56352-69d0-49ae-812b-5b3791c93389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25228e6a-fffb-4312-8d76-1aa8da36d634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b86cb75d3279fd2c37567cfcadc8282",
     "grade": false,
     "grade_id": "cell-e7e47b57728a3497",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Probamos la ingesta de un fichero\n",
    "\n",
    "**Ejercicio 1 (2 puntos)**. Ingestar el fichero **`2023-01-01.json`** utilizando el motor de ingesta completo. Debe crearse el objeto de la clase MotorIngesta y utilizar el método `ingesta_fichero`, dejando el resultado en la variable `flights_df`. La variable `flujo_diario` contiene un objeto FlujoDiario inicializado con el path de configuración anterior, y lo vamos a usar ahora solo para leer adecuadamente la configuración y pasársela al objeto `motor_ingesta` como el argumento config.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d8959acc-c951-4e9e-97a7-f474daf2318e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34de26b267263009f0b64630904f826e",
     "grade": false,
     "grade_id": "ingesta_fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from motor_ingesta.flujo_diario import FlujoDiario\n",
    "from motor_ingesta.motor_ingesta import MotorIngesta\n",
    "import json\n",
    "\n",
    "path_config_flujo_diario = \"/dbfs/FileStore/config/config.json\"  # ruta del fichero config.json\n",
    "path_json_primer_dia = \"dbfs:/FileStore/2023_01_01.json\"  # Spark path para el JSON (corrigiendo el nombre)\n",
    "\n",
    "# Aseguramos que el config tiene el path correcto para timezones\n",
    "with open(path_config_flujo_diario, 'r') as f:\n",
    "    config = json.load(f)\n",
    "config['timezones_path'] = 'dbfs:/FileStore/timezones.csv'  # Spark path\n",
    "with open(path_config_flujo_diario, 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "# Ahora inicializamos FlujoDiario y MotorIngesta\n",
    "flujo_diario = FlujoDiario(path_config_flujo_diario)\n",
    "motor_ingesta = MotorIngesta(flujo_diario.config)\n",
    "flights_df = motor_ingesta.ingesta_fichero(path_json_primer_dia)\n",
    "\n",
    "display(flights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9edc2782-37c0-46aa-a017-ad95b768c914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b779d274c03920f7902ecf1ba28ef59e",
     "grade": true,
     "grade_id": "ingesta-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(flights_df.count() == 15856)\n",
    "print(\"Test 1 OK: flights_df.count() == 15856\")\n",
    "\n",
    "assert(len(flights_df.columns) == 18)\n",
    "print(\"Test 2 OK: len(flights_df.columns) == 18\")\n",
    "\n",
    "dtypes = dict(flights_df.dtypes)\n",
    "assert(dtypes[\"Diverted\"] == \"boolean\")\n",
    "print(\"Test 3 OK: dtypes['Diverted'] == 'boolean'\")\n",
    "\n",
    "assert(dtypes[\"ArrTime\"] == \"int\")\n",
    "print(\"Test 4 OK: dtypes['ArrTime'] == 'int'\")\n",
    "\n",
    "assert(flights_df.schema[\"Dest\"].metadata == {\"comment\": \"Destination Airport IATA code (3 letters)\"})\n",
    "print(\"Test 5 OK: flights_df.schema['Dest'].metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d6d099-5562-4ef0-9f81-c3b83e2696c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3db784f6afa844cf5c8ff545533b6c4a",
     "grade": false,
     "grade_id": "cell-d3f1684d9a8578bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Probamos la función de añadir la hora en formato UTC\n",
    "\n",
    "**Ejercicio 2 (2 puntos)** Probar la función de añadir hora UTC con el DF `flights_df` construido anteriormente. El resultado debe dejarse en la variable `flights_with_utc`. Recuerda que esto no es propiamente un test unitario.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código de la función `aniade_hora_utc` del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09f05b0-e5a7-4a40-911d-21f1f313f4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3674aeefedb405cd73689ee930980846",
     "grade": false,
     "grade_id": "flights-utc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necesarios\n",
    "from motor_ingesta.agregaciones import aniade_hora_utc\n",
    "\n",
    "# Cargar timezones_df que ahora es requerido como parámetro\n",
    "timezones_df = flujo_diario.timezones_df\n",
    "\n",
    "flights_with_utc = aniade_hora_utc(spark, flights_df, timezones_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47e1f5a-14d0-4efe-9c02-b9b1c57bd4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6600ce9efa93f23fcafd8a5c4571af25",
     "grade": true,
     "grade_id": "flights-utc-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "assert(flights_with_utc.where(\"FlightTime is null\").count() == 266)\n",
    "print(\"Test 1 OK: flights_with_utc.where('FlightTime is null').count() == 266\")\n",
    "\n",
    "types = dict(flights_with_utc.dtypes)\n",
    "assert(flights_with_utc.dtypes[18] == (\"FlightTime\", \"timestamp\"))  # FlightTime debe ser la última columna\n",
    "print(\"Test 2 OK: flights_with_utc.dtypes[18] == ('FlightTime', 'timestamp')\")\n",
    "\n",
    "first_row = flights_with_utc.where(\"OriginAirportID = 12884\").select(F.min(\"FlightTime\").cast(\"string\").alias(\"FlightTime\")).first()\n",
    "assert(first_row.FlightTime == \"2023-01-01 10:59:00\")\n",
    "print(\"Test 3 OK: first_row.FlightTime == '2023-01-01 10:59:00'\")\n",
    "\n",
    "display(flights_with_utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "557e1cb5-c0f8-4f68-99c4-545aad0efbd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4b7d83307caa7e39620489749764e78",
     "grade": false,
     "grade_id": "cell-068b74be5a0b9aaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Probamos la función de añadir las columnas con la hora del siguiente vuelo, su aerolínea y el intervalo de tiempo transcurrido\n",
    "\n",
    "**Ejercicio 3 (2.5 puntos)** Invocar a la función de añadir intervalos por aeropuerto, partiendo de la variable `flights_with_utc` del apartado anterior, dejando el resultado devuelto por la función en la variable `df_with_next_flight` cacheada.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código de la función `aniade_intervalos_por_aeropuerto` en el paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2fc08a09-7a92-40e7-a62e-76e07b597615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54149f06e69c8b3d8f8f356c4af06fd9",
     "grade": false,
     "grade_id": "flights-intervalos",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from motor_ingesta.agregaciones import aniade_intervalos_por_aeropuerto\n",
    "\n",
    "df_with_next_flight = aniade_intervalos_por_aeropuerto(flights_with_utc).cache()\n",
    "\n",
    "display(df_with_next_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53380761-c028-4cc6-91ad-be60063d158b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e89190d3c04bf02b372ce8df78f18be9",
     "grade": true,
     "grade_id": "flights-intervalos-test",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(df_with_next_flight.dtypes[19] == (\"FlightTime_next\", \"timestamp\"))\n",
    "print(\"Test 1 OK: df_with_next_flight.dtypes[19] == ('FlightTime_next', 'timestamp')\")\n",
    "\n",
    "assert(df_with_next_flight.dtypes[20] == (\"Airline_next\", \"string\"))\n",
    "print(\"Test 2 OK: df_with_next_flight.dtypes[20] == ('Airline_next', 'string')\")\n",
    "\n",
    "assert(df_with_next_flight.dtypes[21] == (\"diff_next\", \"bigint\"))\n",
    "print(\"Test 3 OK: df_with_next_flight.dtypes[21] == ('diff_next', 'bigint')\")\n",
    "\n",
    "first_row = df_with_next_flight.where(\"OriginAirportID = 12884\")\\\n",
    "                               .select(F.col(\"FlightTime\").cast(\"string\"), \n",
    "                                       F.col(\"FlightTime_next\").cast(\"string\"), \n",
    "                                       F.col(\"Airline_next\"),\n",
    "                                       F.col(\"diff_next\")).sort(\"FlightTime\").first()\n",
    "\n",
    "assert(first_row.FlightTime_next == \"2023-01-01 16:36:00\")\n",
    "print(\"Test 4 OK: first_row.FlightTime_next == '2023-01-01 16:36:00'\")\n",
    "\n",
    "assert(first_row.Airline_next == \"9E\")\n",
    "print(\"Test 5 OK: first_row.Airline_next == '9E'\")\n",
    "\n",
    "assert(first_row.diff_next == 20220)\n",
    "print(\"Test 6 OK: first_row.diff_next == 20220\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf04fa9e-a2de-4547-935d-3e44bdfb78cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b2fd2f171470ce5d27ba06300f08677",
     "grade": false,
     "grade_id": "cell-a3539980f6a4e471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Corregimos que el último vuelo de cada aeropuerto y cada día tiene valor nulo en las 3 columnas `_next`\n",
    "\n",
    "**Ejercicio 4 (2.5 puntos)**\n",
    "\n",
    "Tal como está implementada la lógica del flujo diario, el último vuelo de cada día no tendrá informada la columna FlightTime_next porque no se dispone todavía de datos del día siguiente. Se pide **corregir este comportamiento** para solucionar los valores nulos, modificando el código del método `procesa_diario` de manera que, antes de escribir los datos del día actual, se hayan corregido las tres columnas `_next` en los datos del día anterior al que estamos ingestando. Una manera simple (aunque no necesariamente óptima) de conseguirlo es:\n",
    "* Leer de la tabla la partición que se escribió el día previo, si existiera dicha tabla y dicha partición.\n",
    "* Añadir al DF devuelto por `aniade_hora_utc` las 3 columnas que le faltan para tener la misma estructura que la tabla, que son `FlightTime_next`, `Airline_next` y `diff_next` (pueden ser en ese orden si la función `aniade_intervalos_por_aeropuerto` se ha implementado para añadirlas en ese orden), pero sin darles valor (con valor None, convirtiendo cada columna al tipo de dato adecuado para que después encaje con la tabla existente).\n",
    "* Unir el DF del día previo y el que acabamos de calcular\n",
    "* Invocar a `aniade_intervalos_por_aeropuerto` pasando como argumento el DF resultante de la unión.\n",
    "\n",
    "Aparte de un test unitario (que se deja como optativo pero sin puntuación), la manera de comprobar el funcionamiento será invocar a `procesa_diario` del flujo diario, con los ficheros de dos días consecutivos, y después comprobar lo que se ha escrito en la tabla tras la ingesta del segundo fichero. Lo probaremos con los días 1 y 2 de enero de 2023.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92f0214-7ef9-43ca-861f-1ad9083fba7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc3bf8f53f02529c5da731e260b8c57",
     "grade": false,
     "grade_id": "procesa-diario",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_json_segundo_dia = \"dbfs:/FileStore/2023_01_02.json\"  # Spark path del fichero 2023_01_02.json\n",
    "\n",
    "# Invoca al método procesa_diario del flujo con el path del fichero 2023-01-01.json\n",
    "flujo_diario.procesa_diario(path_json_primer_dia)\n",
    "\n",
    "# Después invoca al método de nuevo con el path del fichero 2023-01-02.json\n",
    "flujo_diario.procesa_diario(path_json_segundo_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05245ade-1b16-4307-947e-729350d01eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b967ae19d8e854ebe370fd146fd86f68",
     "grade": true,
     "grade_id": "tests-unitarios",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vuelos = spark.read.table(\"default.flights\").sort(\"Origin\", \"FlightTime\")\n",
    "assert(vuelos.count() == 33931)\n",
    "print(\"Test 1 OK: vuelos.count() == 33931\")\n",
    "row = vuelos.where(\"FlightDate = '2023-01-01' and  Origin = 'ABE' and DepTime = 1734\").first()\n",
    "assert(row.diff_next == 44220)\n",
    "print(\"Test 2 OK: row.diff_next == 44220\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ec7cf55-a683-4707-a965-2b1c079a5614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "603d2e3dc558735a0429356e1a91c287",
     "grade": false,
     "grade_id": "cell-84b4251b85b7176b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Ejercicio opcional\n",
    "\n",
    "**Ejercicio opcional (1 punto)** Completa los cuatro tests unitarios que encontrarás en el fichero `test_ingesta.py`. No tienes que escribir más código en este notebook.\n",
    "\n",
    "- Se podrá optar a una calificación final de hasta 9.0 puntos sin resolver este ejercicio."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "actividad_spark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
